# Small Language Model (SLM) from Scratch  

This repository contains an implementation of a **Small Language Model (SLM)** built from scratch using Python. The project demonstrates the complete pipeline for constructing and training a simple language model, covering dataset preparation, tokenization, model design, training, and inference.  

---

## Features  
- **Dataset Creation** â€“ Build a text dataset for model training.  
- **Tokenization** â€“ Convert raw text into numerical tokens.  
- **Input-Target Pairs** â€“ Generate training sequences for next-token prediction.  
- **SLM Architecture** â€“ Implement embedding layers, neural network blocks, and output heads.  
- **Pre-training** â€“ Train the model on tokenized data with supervised learning.  
- **Inference** â€“ Generate text sequences using the trained model.  

---

## ðŸ“š References  
- Vaswani et al., *Attention Is All You Need* (2017)  
- Dr. Raj Dandekar â€“ *Building a Small Language Model from Scratch*
